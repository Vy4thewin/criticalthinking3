---
title: "final_project"
author: "Jose Rodriguez, Vyanna Hill, Christian Uriostegui"
date: "2023-12-09"
output: html_document
---

```{r Load-Libraries, include=FALSE}
library(tidyverse) #data wrangling
library(lubridate) #data wrangling - date formatting
library(MASS) #box-cox function
library(car)
library(fastDummies)
library(forecast)
```

### Data Preparation

```{r Load-Dataset}
data_url <- "https://raw.githubusercontent.com/Vy4thewin/criticalthinking3/main/raw_appstore_data.csv"
raw.app.data <-read.csv(data_url)
```

```{r}
#print(sapply(raw.app.data, class))
```

##### Data Subset:  
The data was pre-processed using the tidyverse package. First, a column subset was taken which yielded a total of 18 variables: 17 of those being predictors (x) and one target variable (y). The target variable, also known as the response variable, is 'Rating'. 'This column 'Rating' contains the accumulated average score of a given app between 0 and 5 stars.

##### Missing data:  

Missing data was handled by removing NA values. Two reasons prompted the omission of missing values: 1) removing missing values only removed about 5% of observations, 2) It is not known if the missing data points are missing completely at random (MCAR), hence imputation could introduce bias to the model.

```{r}
#Removed columns that are not fit for a linear regression model. (i.e: observation identifying columns). 'Currency' as there was little variation in value

# BRL   CAD   EUR   GBP   INR   KRW   USD   VND   XXX 
#     1     1     1     1     2     1 28963     1   582

#Create list of columns with blank values
blank_val_cols <- c("Released", "Last.Updated", "Minimum.Android")

#Create list of date cols
date_val_cols <- c("Released", "Last.Updated")

#Create list of columns to convert to factor type
factor_list <- c("Category", "Content.Rating")

app.data <- raw.app.data |>
  dplyr::select(-c(Ã¯..App.Name, App.Id, Developer.Id, Developer.Website, Developer.Email, Privacy.Policy, Scraped.Time, Currency)) |>
  mutate_at(blank_val_cols, ~na_if(., "")) |> #convert blank values to NA to be better handled by na.omit()
  na.omit()|> #drops 1577 observations (approx. 35 of the original dataset)
  mutate_at(factor_list, factor) |> #convert cols to factor type
  mutate(Size = str_replace_all(Size, "[^0-9]","")) |> #remove non digits from 'Size' col
  mutate(Size = as.numeric(Size)) |>
  filter(!is.na(Size)) |>
  mutate(Installs = str_replace_all(Installs, "[^0-9]","")) |> #clean 'Installs' col
  mutate(Installs = as.numeric(Installs)) |>
  mutate(across(all_of(date_val_cols), ~mdy(.), .names = "{.col}")) #use lubridate to convert character cols to date format
```

##### Distribution of Numeric Variables:  
All numerical variables were found to be highly skewed. Transformations such as box-cox, logarithmic and square-root were explored. However, they did not normalize the data. Consequently, several assumptions are violated: 1) Normality, 2) linearity. Ultimately, this leads to assuming that the data exhibits complex relationships where a Nonparametric Regression could be the best choice. 

```{r Checking Variable Distribution}
library(ggpubr)

plot.func <- function(data, col){
  #compute mean for Rating
  mean_rating <- data |>
  pull(col) |>
  mean() |>
  signif(6)
  
  var.dist.plt <- ggplot(
    data=app.data, aes(x=data[[col]])) +
    geom_density()+
    labs(y = "Density", x = data[[col]]) +
    geom_vline(xintercept=mean_rating, linewidth=1.2, color= "red")
  
  return (var.dist.plt)
}

g1 <- plot.func(app.data, "Rating")
g2 <-plot.func(app.data, "Rating.Count")
g3 <- plot.func(app.data, "Installs")
g4 <- plot.func(app.data, "Minimum.Installs")
g5 <- plot.func(app.data, "Maximum.Installs")
g6 <- plot.func(app.data, "Price")
g7 <- plot.func(app.data, "Size")

plt <- ggarrange(g1,g2,g3,g4,g5,g6,g7,ncol =3, nrow = 3)
plt
```

```{r Variable Transformations}
#Using box-cox transformations (MASS Package)
lamb.Rating <- boxcox((app.data$Rating+1)~1)
lamb.Rating.Count <- boxcox((app.data$Rating.Count+1)~1)


#retrieving the exact lambda for transformation
lamb.Rating <- lamb.Rating$x[which.max(lamb.Rating$y)]#0.14
lamb.Rating.Count <- lamb.Rating.Count$x[which.max(lamb.Rating.Count$y)]#-0.22


#app.data <- app.data %>% mutate(Rating = (Rating^lamb.Rating-1/lamb.Rating))
app.data <- app.data%>%mutate(Rating=log(Rating+1))
app.data <- app.data%>%mutate(Rating.Count=sqrt(Rating.Count+1))

```

```{r Response Variable}
# lambda.Rating <- BoxCox.lambda(app.data$Rating)
# app.data$Rating = BoxCox(app.data$Rating, lambda.Rating)
# 
# 
# #compute mean for Rating
# mean_rating <- app.data |>
#   pull(Rating) |>
#   mean() |>
#   signif(6)
# 
# 
# var.dist.plt <- ggplot(
#   data=app.data, aes(x=Rating)) +
#   geom_density()+
#   labs(y = "Density", x = "Rating")# +
#   #geom_vline(xintercept=mean_rating, linewidth=1.2, color= "red")
# 
# plot(var.dist.plt)
```

##### Categorial Variables:  
Categorical variables can easily be handled by most R modeling functions. As a caveat some of modeling functions require categorical predictors to be of class factor. During the dplyr data wrangling segment, all categorical variables were converted to factor type using a list.


##### Checking for Multi-Collinearity:  
'Maximum.Installs' and 'Minimum.Installs' were found to be highly correlated to 'Installs'.

```{r echo=FALSE}
library(corrplot)
#checking for highly correlated variables
numeric.data <- app.data %>%
  select_if(is.numeric)


corrplot(cor(numeric.data),method = "number",type="lower", tl.srt = .71,number.cex=0.75)

```

### Testing GAM Modeling

The team decided to shift the focus from linear models towards Generalized Additive Models (GAM), as the previous linear model attempts produced poor results. In the first model below, the R^2 value increase to 30% with 30% of the deviance accounted. This improvement in the results shows GAM may be a better regression model for fitting the data provided.

The team did notice the R^2 increase with a smoothing function on Rating count. Rating count distribution is highly skewed,as many cases are below <10. The smoothing function provided additional support in shifting the best fit around the feature to a better angle.

```{r Testing NonParametric Model}
library(mgcv)

gam.mod <- gam(Rating ~  Category + 
                 s(Rating.Count) #+
                 #Installs +
                 #Minimum.Installs +
                 #Maximum.Installs +
                 #Free + 
                 #Price + 
                 #Size +
                 #Minimum.Android +
                 #Released +
                 #Last.Updated +
                 #Content.Rating +
                 #Ad.Supported + 
                 #In.App.Purchases +
                 #Editors.Choice
               , data = app.data)

summary(gam.mod)
par(mfrow = c(2, 2)) 
plot(gam.mod)

#checking gam stats
gam.check(gam.mod)
```


### Introducing shrinking and other smoothing techinques

From the data exploration, the team is aware of the non-linearity of the features in the data set. The current features may not have a linear fit, but its best fit shape can be a polynomial. Using splines, its adjusting the the line with penalties towards mse; which makes the shape more linear. In order to capture the majority of the points in the data set, the best fit line may take the shape of different curves to better fit the response Rating. The team use shrinking smoothed in the features provided as it lessens the penalty.

For the models below, the team use splines with a mix of basis. In addition, the method REML (Restricted Maximum Likelihood) was applied as the model has random effects in the features .For Model two, the cubic spline is applied for the features Installs and Rating.counts. These additional splines boosted the R^2 of the model to 72% with its deviance at 72%. In the plots below, the splines of rating count have a closer fit onto the feature than the previous iteration. Checking the gam, the model did coverage with the terms provided. However, the p-values of the smoothed items are significant. This means the smoothing knots need to be increased as the number of bases are seeing a pattern in the model's residuals.

In the third iteration, the team added more bases to the smoothing function. For the feature Installs, the increases bases improved the p-value out of significance. Rating.count p-value still broke the null hypothesis, which points the feature will need more base functions to correct the null. 

```{r}
#reviewing previous model, apply cubic splines and updating method to reml
gam1<- gam(Rating ~s(Rating.Count,bs ='cs') +
                 s(Installs,bs ='cs')+Category+Ad.Supported+In.App.Purchases
               , data = app.data,method = "REML")

summary(gam1)
par(mfrow = c(2, 2)) 
plot(gam1)

par(mfrow = c(2, 2)) 
gam.check(gam1)


#Increasing the basis functions in the smoothing items
gam2<- gam(Rating ~s(Rating.Count,bs="cs",k=100) +
                 s(Installs,bs="cs",k=15)+Category+Ad.Supported+In.App.Purchases, data = app.data,method = "REML")
summary(gam2)
gam.check(gam2)
```


The fourth version of the gam model adds the penalty to the model. There was not a big change the results from the previous.

In the next model, the rating count smoothing was removed and editors choice, content rating, and last updated to the model. Although the R^2 dropped to 52%, the fitted vs actual plot appears more normal. Checking the AIC and a few other metrics, it does appear the fourth model has the advantage than the model with the higher log likelihood and lower deviance.

```{r} 
#Increasing the basis functions in rating count and add extra penalty
gam3<- gam(Rating ~s(Rating.Count,bs="cs",k=100) +
                 s(Installs,bs="cs",k=15)+Category+Ad.Supported+In.App.Purchases+Price, data = app.data,method = "REML",select=TRUE)

summary(gam3)
gam.check(gam3)

#remove smoothing on rating count and add in editors choice
gam4<- gam(Rating ~Rating.Count+s(Installs,bs="cs",k=15)+Category+Ad.Supported+In.App.Purchases+Editors.Choice+Content.Rating+Last.Updated, data = app.data,method = "REML")
summary(gam4)
gam.check(gam4)

#check AIC 
AIC(gam3,gam4)

library(broom)

glance(gam3)
glance(gam4)
```



```{r}
app.data%>%ggplot(aes(x=Rating,y=Rating.Count))+geom_histogram()
app.data%>%ggplot(aes(x=Rating,y=Price))+geom_histogram()


```

