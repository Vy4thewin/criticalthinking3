 ---
title: "Data 621 Homework 3"
author: "Critical Thinking Group 3: Vyannna Hill, Jose Rodriguez, and Christian Uriostegui"
date: "`r Sys.Date()`"
output:
  prettydoc::html_pretty:
    theme: architect
    highlight: github
---

```{r warning=FALSE, include=FALSE}
#importing data sets
library(MASS)
library(tidyverse)
library(ggpubr)
library(corrplot)
library(broom)

training_data<-read.csv("https://raw.githubusercontent.com/Vy4thewin/criticalthinking3/main/crime-training-data_modified.csv")
testing_data<-read.csv("https://raw.githubusercontent.com/Vy4thewin/criticalthinking3/main/crime-evaluation-data_modified.csv")
```


### Data Exploration

#### Introduction to the data set

For this assignment, the team will review the crime data set. The team will create a regression model that will predict the risk assessment of a particular neighborhood.

Let's review the data below:

Variable Name | Definition | Value
------------- | ------------- | -----------|
zn | proportion of residential land zoned for large lots (over 25000 square feet)|predictor variable
indus| proportion of non-retail business acres per suburb| predictor variable
chas| a dummy var. for whether the suburb borders the Charles River (1) or not (0)|predictor variable
nox| nitrogen oxides concentration (parts per 10 million)| predictor variable
rm |average number of rooms per dwelling| predictor variable
age| proportion of owner-occupied units built prior to 1940| predictor variable
dis| weighted mean of distances to five Boston employment centers| predictor variable
rad| index of accessibility to radial highways| predictor variable
tax| full-value property-tax rate per $10,000 |predictor variable
ptratio| pupil-teacher ratio by town |predictor variable
lstat| lower status of the population (percent) |predictor variable
medv| median value of owner-occupied homes in $1000s |predictor variable
target| whether the crime rate is above the median crime rate (1) or not (0)|response variable

Reviewing the training set, the data set has 466 observations and 12 predictor variables in the data set. There is no missing values in the training set, so there is no further steps on imputation. However, the mean and median of a few variables below look strange. Let's review a visualization of the distribution of those values below.


```{r warning=FALSE}
#count of training set
sprintf("Number of observations: %1.f",count(training_data))

#summary of the data set
summary(training_data)

#doubling checking there's no NAs
colSums(is.na(training_data))
```
#### Running into heavily skewed data

Looking into the density plots of the predictor variables, only the average number of rooms variable has a normal distribution. For the variable "zn", the data is very right skewed. This can represent this location does not have a lot of large residential plots; which could mean the area does not see larger apartments or luxury houses. 

Another questionable variable is "rad", which is the index of accessibility to radial highway. This definition measures the dependence of a car in a location^(https://www.sciencedirect.com/science/article/pii/S0966692323000388). The scoring of rad in this data set ranks this location to be poorly accessible as majority of the observations ranks accessibility at 25.

Possible paths for the heavily skewed may need a transformation, but let's see if there are predictors that are multi-collinear that can be  removed pre-transformation!

```{r}
#Look at the distribution of all predictor variables
g1<-training_data%>%ggplot(aes(x=zn))+geom_histogram(bins=25)+theme_classic()
g2<-training_data%>%ggplot(aes(x=indus))+geom_histogram(bins=25)+theme_classic()
g3<-training_data%>%ggplot(aes(x=chas))+geom_histogram(bins=25)+theme_classic()
g4<-training_data%>%ggplot(aes(x=nox))+geom_histogram(bins=25)+theme_classic()
g5<-training_data%>%ggplot(aes(x=rm))+geom_histogram(bins=25)+theme_classic()
g6<-training_data%>%ggplot(aes(x=age))+geom_histogram(bins=25)+theme_classic()
g7<-training_data%>%ggplot(aes(x=dis))+geom_histogram(bins=25)+theme_classic()
g8<-training_data%>%ggplot(aes(x=rad))+geom_histogram(bins=25)+theme_classic()
g9<-training_data%>%ggplot(aes(x=tax))+geom_histogram(bins=25)+theme_classic()
g10<-training_data%>%ggplot(aes(x=ptratio))+geom_histogram(bins=25)+theme_classic()
g11<-training_data%>%ggplot(aes(x=lstat))+geom_histogram(bins=25)+theme_classic()
g12<-training_data%>%ggplot(aes(x=medv))+geom_histogram(bins=25)+theme_classic()
ggarrange(g1,g2,g3,g4,g5,g6,g7,g8,g9,g10,g11,g12,nrow = 3,ncol = 4)
```

#### Reviewing for multi-collinearity 

In review of our predictor variables, rad and tax have a high correlation of 0.91 compared to other variables. The team can remove rad variable from the preliminary regression model as tax variable requires less transformations. There are some moderately correlated variables in the graph below (i.e nox&dis,indus&nox), but the team can investigate in model building if variable removal is necessary. 

```{r}
#checking for multi-collinearity 
corrplot(cor(training_data[,1:12]),method = "number",type="lower", tl.srt = .71,number.cex=0.75)

```
### Data Preparation 

From the data exploration, it was found a few features are heavily skewed. However, this is a logistic model and not a linear model. There is no assumption of a normal distribution like a linear regression model will assume.

* The assumptions for a logistic regression are...
  + No Multi-collinearity 
  + residuals are independent
  + large sample size
  + linear relationship between predictors and logit of Y
  
  
The most useful transformation is handling the highly skewed features for the model.

Zn, tax, pratio, nox, and indus will need transformations before the feature selection. ZN is very fragment in its distribution, so it might benefit from a polynomial transformation. It is a quicker transformation than creating dummy variables for all quartiles in zn.

```{r}
#Creating a polynomial term from zn for a more stable predictor variable
train.set<-training_data%>%mutate_at(c(1),~poly(.,2))

#seeing the difference in distribution
g1<-training_data%>%ggplot(aes(x=zn))+geom_density()+theme_classic()
g2<-train.set%>%ggplot(aes(x=zn[,"2"]))+geom_density()+theme_classic()
ggarrange(g1,g2)

```

We can determine the other feature's transformation through box cox!

```{r}
#Performing box cox on the predictors and retrieving their lambdas
lamb.indus<-boxcox(training_data$indus~1)
lamb.nox<-boxcox(training_data$nox~1)
lamb.pratio<-boxcox(training_data$ptratio~1)
lamb.tax<-boxcox(training_data$tax~1)

lamb.indus<-lamb.indus$x[which.max(lamb.indus$y)]
lamb.nox<-lamb.nox$x[which.max(lamb.nox$y)]
lamb.pratio<-lamb.pratio$x[which.max(lamb.pratio$y)]
lamb.tax<-lamb.tax$x[which.max(lamb.tax$y)]

#indus is near .5 so it will need sqrt(x)
train.set<-train.set%>%mutate(indus=sqrt(indus))

#nox is near -1 so it will get 1/x
train.set<-train.set%>%mutate(nox=1/nox)

#pratio will be ploy like zn
train.set<-train.set%>%mutate(ptratio=poly(ptratio,2))

#tax will get 1/ sqrt(x)
train.set<-train.set%>%mutate(tax=1/sqrt(tax))
```

Now, the team can use the newly transform variables in the variable selection process!

### Build Models

In all three models, we're going to assigning the variable target as our dependent value, and every other variable as our predictors. We want to see what variables, such as zn, indicus, and chas, have on target. Target tells us whether the crime rate in the area is high or low. If a coefficient is positive, this tell us that the higher the value of the variable, the higher the odds of Target to be 1 - which tell us the crime rate in the area is high. The opposite occurs when coefficient is negative. 



### Model Fit 1: Full Baseline Model

Our residuals in the baseline model, which contains the original dataset without modifications, ranges from -1.8464 to 3.4665.

*Positive variables*
The variables chas, and lstat have positive coefficients. The higher these values, the higher the chances that the crime rate in the area is high. In other words if the area is near the Charles River,  or if there is a higher concentration of population that is on the lower end of socio economic status, they will likely be in a high crime rate area. These variables are not statistically significant. 

The coefficients align with what we assume to be key identifiers of the crime rate of the area. While we are unsure why the Charles River has a positive coefficient, one can assume the it's likely going to be a high crime area.If there is a high concentration of poverty, then there will most likely be crime and so the positive coefficient for lstat also makes sense. 

*Negative variables*
The variables zn, indus, and rm have negative coefficients. The higher these values, the lower the chances that the crime rate in the area is high. If the proportion of residential land zoned for large lots is large, if the proportion of non-retail business acres per suburb is large, or if the average number of rooms per dwelling is large then the more likely they will be in a low crime rate area, These variables are not statistically significant  

These variables aren't as intuitive as when looking at the variables with positive coefficients. This tells us that areas with larger residential zoning (more open space), areas with non-retail businesses (local businesses) and average rooms, the more likely there will be a low crime area. These amenities likely offer more economic opportunities (jobs), more space and are most probable of being occupant by affluent families  

*Significant values*
The variables nox, age, dis, rad, ptratio, medv and tax are all statistically significant. 

If there is high population (nox), if the building is older (age), if there isn't close proximity to employment centers (dis), if there is close proximity to highways (rad) and high pupil to teacher ration (ptratio) the higher the chances that the crime rate in the area is high. The positive values of these coefficients is fitting because these values indicate negative enviornmental status, and weak economic opportunity, all which typically exist in high crime areas.

Areas with higher property taxes are less likely to be an area with high crime rate.

*AIC and BIC* 

The AIC for this model is 218.0469 and the BIC is 271.9213

```{r, echo=FALSE}
# Baseline model
fit1 <- glm(target ~ ., data = training_data, family = "binomial")

summary(fit1)
```

```{r, echo=FALSE}
glance(fit1)
```

### Model Fit 2: Full Transformed Model

When comparing the transformed model to the baseline model, we can see a wider range when looking at the residuals. 
Model 2 ranged from -1.7292 to 3.7084.

Similarly we can see most of the same significant variables that we see in model 1: nox, dis, rad, ptratio2, and medv
Tax is not statistically significant in this model. 

Interestingly, the nox variable in this model is negative coefficient value unlike the first model which was a positive coefficient value.

The AIC is 220.1605 and BIC is 282.3232 which is slightly larger than the first model. This indicates the first model is a better fit.

```{r, echo=FALSE}
# Our second logistic regression model with transformed variables
fit2 <- glm(target ~ ., data = train.set, family = "binomial")

# Summary of the second model
summary(fit2)
```

```{r, echo=FALSE}
glance(fit2)
```

### Model Fit 3: Stepwise Regression Model

For the final model, we utilize stepwise regression. This is a technique that seeks a parsimonius model, rather than one with a large selection of predictor values. It contains both forward selection and backward elimnation. Forward selection starts with no variables and then adds them one by one. It only adds variables that give the most improvement based on the F-statistic. Backward elimination starts with all the predictors in the model and then constantly iterates to remove the least significant variables until only the most significant predictors remain. 

All the significant variables from model 1 are in model 3: zn, nox, age, dis, rad, tax, ptratio, medv 

We see the absence of variables such as indus, chas and lstat.

We can also see the AIC of 215.3229 and BIC of 252.6205 is lowest amongst all models, making this the most ideal model.

```{r, echo=FALSE}
# The third model with stepwise regression
fit3 <- stepAIC(fit1, direction = "both", trace = FALSE)

# Summary of the third model
summary(fit3)
```

```{r, echo=FALSE}
glance(fit3)
```

### Select Models




